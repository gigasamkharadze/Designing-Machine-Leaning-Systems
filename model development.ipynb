{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model Development\n",
    "Model development is an iterative process. After each iteration, you’ll want to compare your model’s performance against its performance in previous iterations and evaluate how suitable this iteration is for production.\n",
    "\n",
    "## Evaluating ML Models\n",
    "When considering what model to use, it’s important to consider not only the model’s performance, measured by metrics such as accuracy, F1 score, and log loss, but also its other properties, such as how much data, compute, and time it needs to train, what’s its inference latency, and interpretability. For example, a simple logistic regression model might have lower accuracy than a complex neural network, but it requires less labeled data to start, it’s much faster to train, it’s much easier to deploy, and it’s also much easier to explain why it’s making certain predictions."
   ],
   "id": "797ba00e6e966976"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Tips for model selection\n",
    "\n",
    "### Avoid the state-of-the-art trap\n",
    "Do not jump away from simple models to complex models straight into the state-of-the-art models. Start with simple models and then move to complex models.\n",
    "\n",
    "Researchers often only evaluate models in academic settings, which means that a model being state of the art often means that it performs better than existing models on some static datasets. It doesn’t mean that this model will be fast enough or cheap enough for you to implement. It doesn’t even mean that this model will perform better than other models on your data.\n",
    "\n",
    "### Start with the simplest models.\n",
    "“simple is better than complex,” and the principle is applicable to machine learning models too.\n",
    "- Simple models are easier to deploy.\n",
    "- Starting with something simple and adding more complex components step-by-step makes it easier to understand your model and debug it.\n",
    "- The simplest model serves as a baseline to which you can compare your more complex models.\n",
    "\n",
    "### Avoid human biases in selecting models.\n",
    "The choice of a model should be based on the data and the problem you are trying to solve, not on your personal preferences or biases.\n",
    "Some people are attracted to specific architectures or models because they are popular or because they are the ones they are most familiar with. This can lead to suboptimal results.\n",
    "\n",
    "### Evaluate good performance now versus good performance later.\n",
    "The best model now does not always mean the best model two months from now. For example, a tree-based model might work better now because you don’t have a ton of data yet, but two months from now, you might be able to double your amount of training data, and your neural network might perform much better. A simple way to estimate how your model’s performance might change with more data is to use learning curves.\n",
    "\n",
    "### Evaluate trade-offs.\n",
    "There are many trade-offs you have to make when selecting models. Understanding what’s more important in the performance of your ML system will\n",
    "help you choose the most suitable model.\n",
    "- One classic example of trade-off is the false positives and false negatives trade-off.\n",
    "- Another example of trade-off is compute requirement and accuracy.\n",
    "\n",
    "### Understand your model’s assumptions.\n",
    "- Prediction assumption: Every model that aims to predict an output Y from an input X makes the assumption that it’s possible to predict Y based on X.\n",
    "- IID: Neural networks assume that the examples are independent and identically distributed, which means that all the examples are independently drawn from the same joint distribution\n",
    "- Smoothness: If an input X produces an output Y, then an input close to X would produce an output proportionally close to Y.\n",
    "- Tractability: Let X be the input and Z be the latent representation (compressed) of X. Every generative model makes the assumption that it’s tractable to compute the probability P(Z|X).\n",
    "- Boundaries: A linear classifier assumes that decision boundaries are linear.\n",
    "- Conditional independence: A naive Bayes classifier assumes that the attribute values are independent of each other given the class.\n",
    "- Normally distributed: Many statistical methods assume that data is normally distributed."
   ],
   "id": "ec25293e77b98a09"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Ensembles\n",
    "For example, for the task of predicting whether an email is SPAM or NOT SPAM, you might have three different models. The final prediction for each email is the majority vote of all three models.\n",
    "\n",
    "Ensembling methods are less favored in production because ensembles are more complex to deploy and harder to maintain."
   ],
   "id": "54984ae6d711b03"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Bagging\n",
    "Bagging, shortened from bootstrap aggregating, is designed to improve both the training stability and accuracy of ML algorithms. It reduces variance and helps to avoid overfitting.\n",
    "\n",
    "Given a dataset, instead of training one classifier on the entire dataset, you sample with replacement to create different datasets, called bootstraps, and train a classification or regression model on each of these bootstraps.\n",
    "\n",
    "![bagging](./screenshots/bagging.png)\n",
    "\n",
    "A random forest is an example of bagging. A random forest is a collection of decision trees constructed by both bagging and feature randomness, where each tree can pick only from a random subset of features to use."
   ],
   "id": "b535289f4e56a74f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Boosting\n",
    "Sequentially train a series of weak learners, where each learner learns from the mistakes of the previous one. The final prediction is the weighted sum of the predictions of the weak learners.\n",
    "\n",
    "![boosting](./screenshots/boosting.png)\n"
   ],
   "id": "e032f7c39f44bdd9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Stacking\n",
    "\n",
    "Stacking means that you train base learners from the training data then create a meta-learner that combines the outputs of the base learners to output final predictions\n",
    "\n",
    "![stacking](./screenshots/stacking.png)\n"
   ],
   "id": "42dcfcc6f3a8d039"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Experiment Tracking and Versioning\n",
    "During the model development process, you often have to experiment with many architectures and many different models to choose the best one for your problem. It’s important to keep track of all the definitions needed to re-create an experiment and its relevant artifacts. An artifact is a file generated during an experiment—examples of artifacts can be files that show the loss curve, evaluation loss graph, logs, or intermediate results of a model throughout a training process.\n",
    "\n",
    "## Experiment tracking\n",
    "A large part of training an ML model is babysitting the learning processes. Many\n",
    "problems can arise during the training process, including\n",
    "- loss not decreasing\n",
    "- overfitting\n",
    "- underfitting\n",
    " - fluctuating weight values\n",
    " - dead neurons\n",
    "- running out of memory.\n",
    "\n",
    " It’s important to track what’s going on during training not only to detect and address these issues but also to evaluate whether your model is learning anything useful.\n",
    "\n",
    "Following is just a short list of things you might want to consider tracking for each experiment during its training process:\n",
    "\n",
    "- Loss curve\n",
    "- model performance metrics such as accuracy, F1 score, and perplexity\n",
    "- Log of corresponding sample, prediction, and ground truth label.\n",
    "- Speed of the model\n",
    "- System performance metrics such as memory usage and CPU/GPU utilization.\n",
    "- parameter and hyperparameter"
   ],
   "id": "249c3bc8ebd55e27"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Versioning\n",
    "Versioning is the process of assigning unique identifiers to different versions of your code, data, and models. Versioning is important because it allows you to track changes to your code, data, and models over time. It also allows you to reproduce experiments and results by being able to go back to a specific version of your code, data, and models."
   ],
   "id": "f0bcb04f67e95ee8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Distributed Training\n",
    "As models are getting bigger and more resource-intensive, companies care a lot more about training at scale.\n",
    "When your data doesn’t fit into memory, your algorithms for preprocessing, shuffling, and batching data will need to run out of core and in parallel.\n",
    "\n",
    "### Data parallelism\n",
    "You split your data on multiple machines, train your model on all of them, and accumulate gradients. This gives rise to a couple of issues. A challenging problem is how to accurately and effectively accumulate gradients from different machines. As each machine produces its own gradient, if your model waits for all of them to finish a run—synchronous stochastic gradient descent (SGD)—stragglers will cause the entire system to slow down, wasting time and resources.\n",
    "\n",
    "### Model parallelism\n",
    "With data parallelism, each worker has its own copy of the whole model and does all the computation necessary for its copy of the model. Model parallelism is when different components of your model are trained on different machines.\n",
    "\n",
    "![model-parallelism](./screenshots/model-parallelism.png)\n",
    "\n",
    "**Pipeline parallelism** is a clever technique to make different components of a model on different machines run more in parallel. There are multiple variants to this, but the key idea is to break the computation of each machine into multiple parts. When machine 1 finishes the first part of its computation, it passes the result onto\n",
    "machine 2, then continues to the second part, and so on.\n",
    "\n",
    "![pipeline-parallelism](./screenshots/pipeline-parallelism.png)"
   ],
   "id": "3ab677d7adaa5090"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Baseline Models\n",
    "These are the models that you compare your model against. They are usually simple models that are easy to implement and understand.\n",
    "\n",
    "1. **Random baseline**: This model predicts a random class for each example. It’s a good baseline to compare your model against.\n",
    "2. **Simple heuristic baseline**: This model uses a simple heuristic to make predictions. For example, if you are predicting whether an email is spam or not, you might use the heuristic that all emails containing the word “Viagra” are spam.\n",
    "3. **Zero rule baseline**: This model always predicts the most common class in the training data. It’s a good baseline to compare your model against because it’s simple and easy to implement.\n",
    "4. **Human baseline**: This model is a human expert who makes predictions on the test data. It’s a good baseline to compare your model against because it tells you how well your model is doing compared to a human expert.\n",
    "5. **Existing model baseline**: This model is an existing model that has been trained on the same data. It’s a good baseline to compare your model against because it tells you how well your model is doing compared to an existing model."
   ],
   "id": "27844a241225b769"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Evaluation Methods\n",
    "\n",
    "### Perturbation tests\n",
    "Perturbation tests are a way to evaluate the robustness of a model. The idea is to introduce small changes to the input data and see how much the output changes. If the output changes a lot, it means that the model is not robust.\n",
    "\n",
    "### Invariance tests\n",
    "Similar to perturbation tests, invariance tests are a way to evaluate the robustness of a model. The idea is to introduce small changes to the input data and see how much the output changes. There are features that **must not cause changes in the model output**. For example, if your are predicting the Covid-19 cases, the model should not change its prediction if you change the name of the patient.\n",
    "\n",
    "### Directional expectation tests\n",
    "When developing a model to predict housing prices, keeping all the features the same but increasing the lot size shouldn’t decrease the predicted price, and decreasing the square footage shouldn’t increase it.\n",
    "\n",
    "### Model calibration\n",
    "Suppose user A watches romance movies 80% of the time and comedy 20% of the time. If your recommender system shows exactly the movies A will most likely watch, the recommendations will consist of only romance movies because A is much more likely to watch romance than any other type of movies. You might want a more calibrated system whose recommendations are representative of users’ actual watching habits. In this case, they should consist of 80% romance and 20% comedy.\n",
    "\n",
    "To measure a model's calibration, a simple method is counting: you count the number of times your model outputs the probability X and the frequency Y of that prediction coming true, and plot X against Y.\n",
    "\n",
    "- sklearn.calibration.CalibratedClassifierCV\n",
    "\n",
    "### Confidence measurement\n",
    "A model’s confidence is how sure it is about its predictions. For example, if a model predicts that a patient has a 70% chance of having cancer, it’s 70% confident in that prediction. Confidence is important because it can help you decide how much you trust a model’s predictions.\n",
    "\n",
    "### Slice-based evaluation\n",
    "Slicing means to separate your data into subsets and look at your model’s performance on each subset separately.\n",
    "This is useful because it can help you identify biases in your model. For example, if your model performs well on one subset of your data but poorly on another subset, it might be because your model is biased towards the first subset."
   ],
   "id": "1523e605d250f334"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
